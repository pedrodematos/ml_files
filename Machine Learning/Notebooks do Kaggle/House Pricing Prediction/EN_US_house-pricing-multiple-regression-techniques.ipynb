{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Pricing: Automatic Data preprocessing & Modeling Techniques Selection using Pipelines\n\nNotebook written by Pedro de Matos Gon√ßalves"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport collections\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500) # Setting pandas to display a N number of columns\npd.set_option('display.max_rows', 10) # Setting pandas to display a N number rows\npd.set_option('display.width', 1000) # Setting pandas dataframe display width to N\nimport matplotlib.pyplot as plt # data visualization library\nimport plotly.graph_objs as go # interactive plotting library\nfrom IPython.display import display # display from IPython.display\nfrom itertools import cycle # function used for cycling over values\n\n\n# Libraries used for Modeling\nfrom scipy import stats\nfrom category_encoders import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, StackingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom xgboost import XGBRegressor, plot_importance as plot_importance_xgb\nfrom lightgbm import LGBMRegressor, plot_importance as plot_importance_lgbm\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the data and displaying some rows\ndf = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n\ndisplay(df.head(10))","execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle  OverallQual  OverallCond  YearBuilt  YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType  MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1  BsmtFinSF1 BsmtFinType2  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF Heating HeatingQC CentralAir Electrical  1stFlrSF  2ndFlrSF  LowQualFinSF  GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  KitchenAbvGr KitchenQual  TotRmsAbvGrd Functional  Fireplaces FireplaceQu GarageType  GarageYrBlt GarageFinish  GarageCars  GarageArea GarageQual GarageCond PavedDrive  WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea PoolQC  Fence MiscFeature  MiscVal  MoSold  YrSold SaleType SaleCondition  SalePrice\n0   1          60       RL         65.0     8450   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl      CollgCr       Norm       Norm     1Fam     2Story            7            5       2003          2003     Gable  CompShg     VinylSd     VinylSd    BrkFace       196.0        Gd        TA      PConc       Gd       TA           No          GLQ         706          Unf           0        150          856    GasA        Ex          Y      SBrkr       856       854             0       1710             1             0         2         1             3             1          Gd             8        Typ           0         NaN     Attchd       2003.0          RFn           2         548         TA         TA          Y           0           61              0          0            0         0    NaN    NaN         NaN        0       2    2008       WD        Normal     208500\n1   2          20       RL         80.0     9600   Pave   NaN      Reg         Lvl    AllPub       FR2       Gtl      Veenker      Feedr       Norm     1Fam     1Story            6            8       1976          1976     Gable  CompShg     MetalSd     MetalSd       None         0.0        TA        TA     CBlock       Gd       TA           Gd          ALQ         978          Unf           0        284         1262    GasA        Ex          Y      SBrkr      1262         0             0       1262             0             1         2         0             3             1          TA             6        Typ           1          TA     Attchd       1976.0          RFn           2         460         TA         TA          Y         298            0              0          0            0         0    NaN    NaN         NaN        0       5    2007       WD        Normal     181500\n2   3          60       RL         68.0    11250   Pave   NaN      IR1         Lvl    AllPub    Inside       Gtl      CollgCr       Norm       Norm     1Fam     2Story            7            5       2001          2002     Gable  CompShg     VinylSd     VinylSd    BrkFace       162.0        Gd        TA      PConc       Gd       TA           Mn          GLQ         486          Unf           0        434          920    GasA        Ex          Y      SBrkr       920       866             0       1786             1             0         2         1             3             1          Gd             6        Typ           1          TA     Attchd       2001.0          RFn           2         608         TA         TA          Y           0           42              0          0            0         0    NaN    NaN         NaN        0       9    2008       WD        Normal     223500\n3   4          70       RL         60.0     9550   Pave   NaN      IR1         Lvl    AllPub    Corner       Gtl      Crawfor       Norm       Norm     1Fam     2Story            7            5       1915          1970     Gable  CompShg     Wd Sdng     Wd Shng       None         0.0        TA        TA     BrkTil       TA       Gd           No          ALQ         216          Unf           0        540          756    GasA        Gd          Y      SBrkr       961       756             0       1717             1             0         1         0             3             1          Gd             7        Typ           1          Gd     Detchd       1998.0          Unf           3         642         TA         TA          Y           0           35            272          0            0         0    NaN    NaN         NaN        0       2    2006       WD       Abnorml     140000\n4   5          60       RL         84.0    14260   Pave   NaN      IR1         Lvl    AllPub       FR2       Gtl      NoRidge       Norm       Norm     1Fam     2Story            8            5       2000          2000     Gable  CompShg     VinylSd     VinylSd    BrkFace       350.0        Gd        TA      PConc       Gd       TA           Av          GLQ         655          Unf           0        490         1145    GasA        Ex          Y      SBrkr      1145      1053             0       2198             1             0         2         1             4             1          Gd             9        Typ           1          TA     Attchd       2000.0          RFn           3         836         TA         TA          Y         192           84              0          0            0         0    NaN    NaN         NaN        0      12    2008       WD        Normal     250000\n5   6          50       RL         85.0    14115   Pave   NaN      IR1         Lvl    AllPub    Inside       Gtl      Mitchel       Norm       Norm     1Fam     1.5Fin            5            5       1993          1995     Gable  CompShg     VinylSd     VinylSd       None         0.0        TA        TA       Wood       Gd       TA           No          GLQ         732          Unf           0         64          796    GasA        Ex          Y      SBrkr       796       566             0       1362             1             0         1         1             1             1          TA             5        Typ           0         NaN     Attchd       1993.0          Unf           2         480         TA         TA          Y          40           30              0        320            0         0    NaN  MnPrv        Shed      700      10    2009       WD        Normal     143000\n6   7          20       RL         75.0    10084   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl      Somerst       Norm       Norm     1Fam     1Story            8            5       2004          2005     Gable  CompShg     VinylSd     VinylSd      Stone       186.0        Gd        TA      PConc       Ex       TA           Av          GLQ        1369          Unf           0        317         1686    GasA        Ex          Y      SBrkr      1694         0             0       1694             1             0         2         0             3             1          Gd             7        Typ           1          Gd     Attchd       2004.0          RFn           2         636         TA         TA          Y         255           57              0          0            0         0    NaN    NaN         NaN        0       8    2007       WD        Normal     307000\n7   8          60       RL          NaN    10382   Pave   NaN      IR1         Lvl    AllPub    Corner       Gtl       NWAmes       PosN       Norm     1Fam     2Story            7            6       1973          1973     Gable  CompShg     HdBoard     HdBoard      Stone       240.0        TA        TA     CBlock       Gd       TA           Mn          ALQ         859          BLQ          32        216         1107    GasA        Ex          Y      SBrkr      1107       983             0       2090             1             0         2         1             3             1          TA             7        Typ           2          TA     Attchd       1973.0          RFn           2         484         TA         TA          Y         235          204            228          0            0         0    NaN    NaN        Shed      350      11    2009       WD        Normal     200000\n8   9          50       RM         51.0     6120   Pave   NaN      Reg         Lvl    AllPub    Inside       Gtl      OldTown     Artery       Norm     1Fam     1.5Fin            7            5       1931          1950     Gable  CompShg     BrkFace     Wd Shng       None         0.0        TA        TA     BrkTil       TA       TA           No          Unf           0          Unf           0        952          952    GasA        Gd          Y      FuseF      1022       752             0       1774             0             0         2         0             2             2          TA             8       Min1           2          TA     Detchd       1931.0          Unf           2         468         Fa         TA          Y          90            0            205          0            0         0    NaN    NaN         NaN        0       4    2008       WD       Abnorml     129900\n9  10         190       RL         50.0     7420   Pave   NaN      Reg         Lvl    AllPub    Corner       Gtl      BrkSide     Artery     Artery   2fmCon     1.5Unf            5            6       1939          1950     Gable  CompShg     MetalSd     MetalSd       None         0.0        TA        TA     BrkTil       TA       TA           No          GLQ         851          Unf           0        140          991    GasA        Ex          Y      SBrkr      1077         0             0       1077             1             0         1         0             2             2          TA             5        Typ           2          TA     Attchd       1939.0          RFn           1         205         Gd         TA          Y           0            4              0          0            0         0    NaN    NaN         NaN        0       1    2008       WD        Normal     118000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>LandSlope</th>\n      <th>Neighborhood</th>\n      <th>Condition1</th>\n      <th>Condition2</th>\n      <th>BldgType</th>\n      <th>HouseStyle</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>RoofStyle</th>\n      <th>RoofMatl</th>\n      <th>Exterior1st</th>\n      <th>Exterior2nd</th>\n      <th>MasVnrType</th>\n      <th>MasVnrArea</th>\n      <th>ExterQual</th>\n      <th>ExterCond</th>\n      <th>Foundation</th>\n      <th>BsmtQual</th>\n      <th>BsmtCond</th>\n      <th>BsmtExposure</th>\n      <th>BsmtFinType1</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinType2</th>\n      <th>BsmtFinSF2</th>\n      <th>BsmtUnfSF</th>\n      <th>TotalBsmtSF</th>\n      <th>Heating</th>\n      <th>HeatingQC</th>\n      <th>CentralAir</th>\n      <th>Electrical</th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>LowQualFinSF</th>\n      <th>GrLivArea</th>\n      <th>BsmtFullBath</th>\n      <th>BsmtHalfBath</th>\n      <th>FullBath</th>\n      <th>HalfBath</th>\n      <th>BedroomAbvGr</th>\n      <th>KitchenAbvGr</th>\n      <th>KitchenQual</th>\n      <th>TotRmsAbvGrd</th>\n      <th>Functional</th>\n      <th>Fireplaces</th>\n      <th>FireplaceQu</th>\n      <th>GarageType</th>\n      <th>GarageYrBlt</th>\n      <th>GarageFinish</th>\n      <th>GarageCars</th>\n      <th>GarageArea</th>\n      <th>GarageQual</th>\n      <th>GarageCond</th>\n      <th>PavedDrive</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>CollgCr</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>2Story</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>VinylSd</td>\n      <td>VinylSd</td>\n      <td>BrkFace</td>\n      <td>196.0</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>PConc</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>GLQ</td>\n      <td>706</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>150</td>\n      <td>856</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>856</td>\n      <td>854</td>\n      <td>0</td>\n      <td>1710</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>Gd</td>\n      <td>8</td>\n      <td>Typ</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Attchd</td>\n      <td>2003.0</td>\n      <td>RFn</td>\n      <td>2</td>\n      <td>548</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>61</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>Gtl</td>\n      <td>Veenker</td>\n      <td>Feedr</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>1Story</td>\n      <td>6</td>\n      <td>8</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>MetalSd</td>\n      <td>MetalSd</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>CBlock</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>Gd</td>\n      <td>ALQ</td>\n      <td>978</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>284</td>\n      <td>1262</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>1262</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1262</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>TA</td>\n      <td>6</td>\n      <td>Typ</td>\n      <td>1</td>\n      <td>TA</td>\n      <td>Attchd</td>\n      <td>1976.0</td>\n      <td>RFn</td>\n      <td>2</td>\n      <td>460</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>298</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>CollgCr</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>2Story</td>\n      <td>7</td>\n      <td>5</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>VinylSd</td>\n      <td>VinylSd</td>\n      <td>BrkFace</td>\n      <td>162.0</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>PConc</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>Mn</td>\n      <td>GLQ</td>\n      <td>486</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>434</td>\n      <td>920</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>920</td>\n      <td>866</td>\n      <td>0</td>\n      <td>1786</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>Gd</td>\n      <td>6</td>\n      <td>Typ</td>\n      <td>1</td>\n      <td>TA</td>\n      <td>Attchd</td>\n      <td>2001.0</td>\n      <td>RFn</td>\n      <td>2</td>\n      <td>608</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>42</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>9</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>RL</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>Crawfor</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>2Story</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>Wd Sdng</td>\n      <td>Wd Shng</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>BrkTil</td>\n      <td>TA</td>\n      <td>Gd</td>\n      <td>No</td>\n      <td>ALQ</td>\n      <td>216</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>540</td>\n      <td>756</td>\n      <td>GasA</td>\n      <td>Gd</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>961</td>\n      <td>756</td>\n      <td>0</td>\n      <td>1717</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>Gd</td>\n      <td>7</td>\n      <td>Typ</td>\n      <td>1</td>\n      <td>Gd</td>\n      <td>Detchd</td>\n      <td>1998.0</td>\n      <td>Unf</td>\n      <td>3</td>\n      <td>642</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>35</td>\n      <td>272</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2006</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>FR2</td>\n      <td>Gtl</td>\n      <td>NoRidge</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>2Story</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>VinylSd</td>\n      <td>VinylSd</td>\n      <td>BrkFace</td>\n      <td>350.0</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>PConc</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>Av</td>\n      <td>GLQ</td>\n      <td>655</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>490</td>\n      <td>1145</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>1145</td>\n      <td>1053</td>\n      <td>0</td>\n      <td>2198</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>Gd</td>\n      <td>9</td>\n      <td>Typ</td>\n      <td>1</td>\n      <td>TA</td>\n      <td>Attchd</td>\n      <td>2000.0</td>\n      <td>RFn</td>\n      <td>3</td>\n      <td>836</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>192</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>12</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>250000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>50</td>\n      <td>RL</td>\n      <td>85.0</td>\n      <td>14115</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>Mitchel</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>1.5Fin</td>\n      <td>5</td>\n      <td>5</td>\n      <td>1993</td>\n      <td>1995</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>VinylSd</td>\n      <td>VinylSd</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Wood</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>GLQ</td>\n      <td>732</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>64</td>\n      <td>796</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>796</td>\n      <td>566</td>\n      <td>0</td>\n      <td>1362</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>TA</td>\n      <td>5</td>\n      <td>Typ</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Attchd</td>\n      <td>1993.0</td>\n      <td>Unf</td>\n      <td>2</td>\n      <td>480</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>40</td>\n      <td>30</td>\n      <td>0</td>\n      <td>320</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>MnPrv</td>\n      <td>Shed</td>\n      <td>700</td>\n      <td>10</td>\n      <td>2009</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>143000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>20</td>\n      <td>RL</td>\n      <td>75.0</td>\n      <td>10084</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>Somerst</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>1Story</td>\n      <td>8</td>\n      <td>5</td>\n      <td>2004</td>\n      <td>2005</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>VinylSd</td>\n      <td>VinylSd</td>\n      <td>Stone</td>\n      <td>186.0</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>PConc</td>\n      <td>Ex</td>\n      <td>TA</td>\n      <td>Av</td>\n      <td>GLQ</td>\n      <td>1369</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>317</td>\n      <td>1686</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>1694</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1694</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>Gd</td>\n      <td>7</td>\n      <td>Typ</td>\n      <td>1</td>\n      <td>Gd</td>\n      <td>Attchd</td>\n      <td>2004.0</td>\n      <td>RFn</td>\n      <td>2</td>\n      <td>636</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>255</td>\n      <td>57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2007</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>307000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>60</td>\n      <td>RL</td>\n      <td>NaN</td>\n      <td>10382</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>IR1</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>NWAmes</td>\n      <td>PosN</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>2Story</td>\n      <td>7</td>\n      <td>6</td>\n      <td>1973</td>\n      <td>1973</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>HdBoard</td>\n      <td>HdBoard</td>\n      <td>Stone</td>\n      <td>240.0</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>CBlock</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>Mn</td>\n      <td>ALQ</td>\n      <td>859</td>\n      <td>BLQ</td>\n      <td>32</td>\n      <td>216</td>\n      <td>1107</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>1107</td>\n      <td>983</td>\n      <td>0</td>\n      <td>2090</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>TA</td>\n      <td>7</td>\n      <td>Typ</td>\n      <td>2</td>\n      <td>TA</td>\n      <td>Attchd</td>\n      <td>1973.0</td>\n      <td>RFn</td>\n      <td>2</td>\n      <td>484</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>235</td>\n      <td>204</td>\n      <td>228</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Shed</td>\n      <td>350</td>\n      <td>11</td>\n      <td>2009</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>200000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>50</td>\n      <td>RM</td>\n      <td>51.0</td>\n      <td>6120</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Inside</td>\n      <td>Gtl</td>\n      <td>OldTown</td>\n      <td>Artery</td>\n      <td>Norm</td>\n      <td>1Fam</td>\n      <td>1.5Fin</td>\n      <td>7</td>\n      <td>5</td>\n      <td>1931</td>\n      <td>1950</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>BrkFace</td>\n      <td>Wd Shng</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>BrkTil</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>952</td>\n      <td>952</td>\n      <td>GasA</td>\n      <td>Gd</td>\n      <td>Y</td>\n      <td>FuseF</td>\n      <td>1022</td>\n      <td>752</td>\n      <td>0</td>\n      <td>1774</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>TA</td>\n      <td>8</td>\n      <td>Min1</td>\n      <td>2</td>\n      <td>TA</td>\n      <td>Detchd</td>\n      <td>1931.0</td>\n      <td>Unf</td>\n      <td>2</td>\n      <td>468</td>\n      <td>Fa</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>90</td>\n      <td>0</td>\n      <td>205</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>4</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n      <td>129900</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>190</td>\n      <td>RL</td>\n      <td>50.0</td>\n      <td>7420</td>\n      <td>Pave</td>\n      <td>NaN</td>\n      <td>Reg</td>\n      <td>Lvl</td>\n      <td>AllPub</td>\n      <td>Corner</td>\n      <td>Gtl</td>\n      <td>BrkSide</td>\n      <td>Artery</td>\n      <td>Artery</td>\n      <td>2fmCon</td>\n      <td>1.5Unf</td>\n      <td>5</td>\n      <td>6</td>\n      <td>1939</td>\n      <td>1950</td>\n      <td>Gable</td>\n      <td>CompShg</td>\n      <td>MetalSd</td>\n      <td>MetalSd</td>\n      <td>None</td>\n      <td>0.0</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>BrkTil</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>GLQ</td>\n      <td>851</td>\n      <td>Unf</td>\n      <td>0</td>\n      <td>140</td>\n      <td>991</td>\n      <td>GasA</td>\n      <td>Ex</td>\n      <td>Y</td>\n      <td>SBrkr</td>\n      <td>1077</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1077</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>TA</td>\n      <td>5</td>\n      <td>Typ</td>\n      <td>2</td>\n      <td>TA</td>\n      <td>Attchd</td>\n      <td>1939.0</td>\n      <td>RFn</td>\n      <td>1</td>\n      <td>205</td>\n      <td>Gd</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2008</td>\n      <td>WD</td>\n      <td>Normal</td>\n      <td>118000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for columns where null values are higher than 50% of it's total\ndf_aux_nulls = [(c, df[c].isna().mean()*100) for c in df]\ndf_aux_nulls = pd.DataFrame(df_aux_nulls, columns=[\"column_name\", \"null_percentage\"])\n\ndf_aux_nulls = df_aux_nulls[df_aux_nulls.null_percentage > 50]\nprint(\"Columns with more than 50% null percentage:\")\ndf_aux_nulls.sort_values(\"null_percentage\", ascending=False) # These are the 3 columns with more than 50% of nulls","execution_count":3,"outputs":[{"output_type":"stream","text":"Columns with more than 50% null percentage:\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"    column_name  null_percentage\n72       PoolQC        99.520548\n74  MiscFeature        96.301370\n6         Alley        93.767123\n73        Fence        80.753425","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>column_name</th>\n      <th>null_percentage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>72</th>\n      <td>PoolQC</td>\n      <td>99.520548</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>MiscFeature</td>\n      <td>96.301370</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Alley</td>\n      <td>93.767123</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>Fence</td>\n      <td>80.753425</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming our target into log scale, to help improve generalization\ndf['SalePrice'] = np.log(df.SalePrice)\n\n# Separating our target\ntarget = df['SalePrice']\n\n# Let's drop the highly null columns and our target from the original dataframe.\ndf.drop(['Id', 'Alley', 'PoolQC', 'Fence', 'MiscFeature', 'SalePrice'], axis=1, inplace=True)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking sell month and year as categorical columns\ndf['YrSold'] = df['YrSold'].astype('object')\ndf['MoSold'] = df['MoSold'].astype('object')\n\n# Now, we separate categorical and numerical column dataframes.\ncategorical_df = df.select_dtypes(include=['object'])\nnumeric_df = df.select_dtypes(exclude=['object'])\n\n# And then, we store the names of the categorical and numerical columns.\ncategorical_features = list(categorical_df.columns)\nnumeric_features = list(numeric_df.columns)\n\nprint(\"Categorical features:\\n\", categorical_features)\nprint(\"\\nNumeric features:\\n\", numeric_features)","execution_count":5,"outputs":[{"output_type":"stream","text":"Categorical features:\n ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n\nNumeric features:\n ['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"\n## Model training & Evaluation functions\n\nAfter all the preprocessing, we are now ready for building and evaluating different Machine Learning models.\n\nFirst, let's create a function responsible for evaluating our regressors on a test set we will create.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def testSetResultsRegressor(regressor, x_test, y_test):\n    predictions = regressor.predict(x_test)\n    \n    results = []\n    \n    mae = mean_absolute_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    rmse = mean_squared_error(y_test, predictions, squared=False)\n    r2 = r2_score(y_test, predictions)\n    \n    results.append(mae)\n    results.append(mse)\n    results.append(rmse)\n    results.append(r2)\n    \n    print(\"\\n\\n#---------------- Test set results (Best Regressor) ----------------#\\n\")\n    print(\"Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R¬≤ Score:\")\n    print(results)\n    \n    return results","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we fit several different data preprocessing, feature selection and modeling techniques inside a Pipeline, to check which group of techniques has better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building a Pipeline responsible for finding best model and it's parameters\ndef defineBestModelPipeline(df, target, categorical_features, numeric_features):\n    \n    # Splitting data into Train and Test\n    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.25, random_state=1)\n    y_train = y_train.to_numpy() # Transforming training targets into numpy arrays\n    y_test = y_test.to_numpy() # Transforming test targets into numpy arrays\n    \n    \n    # Pipeline's data transformations\n    # 1st -> Numeric Transformers (we'll try several different ones)\n    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=10, random_state=1)),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=7)),\n                                            ('scaler', StandardScaler())])\n    \n    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n    \n    \n    # 2nd -> Categorical Transformer\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n    \n    # 3rd -> Different Data Transformation Steps, each one with a different numerical transformation\n    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    \n    \n    # Applying different data transformations in RandomSearchCV to find \n    # the best imputing strategy, the best feature engineering strategy\n    # and the best model with it's parameters\n    pipe = Pipeline(steps=[('data_transformations', data_transformations_1), # Initializing data transformation step by choosing any of the above\n                           ('feature_eng', PCA()), # Initializing feature engineering step by choosing any desired method\n                           ('reg', SVR())]) # Initializing modeling step of the pipeline with any model object\n                           #memory='cache_folder') -> Used to optimize memory when needed\n    \n    \n    \n    # Now, defining the grid of parameters to search for. RandomSearchCV will randomly chose\n    # options for each step inside the dictionaries, and return the best one for us as our final pipeline.\n    params_grid = [\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [KNeighborsRegressor()],\n                     'reg__n_neighbors': stats.randint(1, 10),\n                     'reg__metric': ['minkowski', 'euclidean']},\n\n        \n\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [LinearRegression()]},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [SVR()],\n                     'reg__C': stats.uniform(0.01, 100),\n                     'reg__gamma': stats.uniform(0.01, 100)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [DecisionTreeRegressor()],\n                     'reg__criterion': ['gini', 'entropy'],\n                     'reg__max_features': [None, \"auto\", \"log2\"],\n                     'reg__max_depth': [None, stats.randint(1, 15)]},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [RandomForestRegressor()],\n                     'reg__n_estimators': stats.randint(10, 300),\n                     'reg__max_features': [None, \"auto\", \"log2\"],\n                     'reg__max_depth': stats.randint(1, 9)},\n        \n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [ExtraTreesRegressor()],\n                     'reg__n_estimators': stats.randint(10, 300),\n                     'reg__max_features': [None, \"auto\", \"log2\"],\n                     'reg__max_depth': stats.randint(1, 9)},\n\n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [GradientBoostingRegressor()],\n                     'reg__n_estimators': stats.randint(10, 300),\n                     'reg__learning_rate': stats.uniform(0.01, 1.5),\n                     'reg__max_depth': stats.randint(1, 12)},\n\n        \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [LGBMRegressor()],\n                     'reg__n_estimators': stats.randint(1, 150),\n                     'reg__learning_rate': stats.uniform(0.01, 1.2),\n                     'reg__max_depth': stats.randint(1, 9)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [XGBRegressor()],\n                     'reg__n_estimators': stats.randint(1, 175),\n                     'reg__eta': stats.uniform(0.01, 1.2),\n                     'reg__max_depth': stats.randint(1, 10),\n                     'reg__gamma': stats.uniform(0.01, 1.2)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [StackingRegressor(estimators=[('svr', SVR(C=10, gamma=10)),\n                                                           ('rf', RandomForestRegressor(max_depth=7, max_features=None, n_estimators=20, n_jobs=-1)),\n                                                           ('xgb', XGBRegressor(eta=0.8, gamma=0.5, max_depth=7, n_estimators=30))], \n                                                final_estimator=LinearRegression())]},\n        \n        \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.8)),\n                                     TSNE(n_components=round(x_train.shape[1]*0.7)), \n                                     TSNE(n_components=round(x_train.shape[1]*0.6))],\n                     'reg': [StackingRegressor(estimators=[('lgbm', LGBMRegressor(n_estimators=50, learning_rate=0.6, max_depth=8)),\n                                                           ('etc', ExtraTreesRegressor(max_depth=8, max_features=None, n_estimators=20)),\n                                                           ('gbt', GradientBoostingRegressor(learning_rate=0.9, max_depth=7, n_estimators=20))], \n                                                final_estimator=LinearRegression())]}\n                ]\n    \n    \n    # Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n    metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'r2']\n    \n    best_model_pipeline = RandomizedSearchCV(pipe, params_grid, n_iter=100, scoring=metrics, \n                                             refit='neg_root_mean_squared_error', \n                                             n_jobs=-1, cv=4, random_state=42)\n\n    best_model_pipeline.fit(x_train, y_train)\n    \n    \n    # At last, we check the final results\n    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n    print(\"\\n\\n#---------------- Best Regressor found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n    print(\"\\n\\n#---------------- Best Estimator's average RMSE Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n    \n    return x_train, x_test, y_train, y_test, best_model_pipeline","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the function above, returing train/test data and best model's pipeline\nx_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(df, target, categorical_features, numeric_features)\n\n\n# Checking best model's performance on test data\ntest_set_results = testSetResultsRegressor(best_model_pipeline, x_test, y_test)","execution_count":8,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n  elif pd.api.types.is_categorical(cols):\n","name":"stderr"},{"output_type":"stream","text":"\n\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\n\n ColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('imp',\n                                                  SimpleImputer(strategy='median')),\n                                                 ('scaler', StandardScaler())]),\n                                 ['MSSubClass', 'LotFrontage', 'LotArea',\n                                  'OverallQual', 'OverallCond', 'YearBuilt',\n                                  'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n                                  'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n                                  '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                                  'GrLivArea', 'BsmtFullBath', 'B...\n                                  'LandContour', 'Utilities', 'LotConfig',\n                                  'LandSlope', 'Neighborhood', 'Condition1',\n                                  'Condition2', 'BldgType', 'HouseStyle',\n                                  'RoofStyle', 'RoofMatl', 'Exterior1st',\n                                  'Exterior2nd', 'MasVnrType', 'ExterQual',\n                                  'ExterCond', 'Foundation', 'BsmtQual',\n                                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                  'BsmtFinType2', 'Heating', 'HeatingQC',\n                                  'CentralAir', 'Electrical', 'KitchenQual', ...])])\n\n\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\n\n None\n\n\n#---------------- Best Regressor found in RandomSearchCV  ----------------#\n\n GradientBoostingRegressor(learning_rate=0.07967561907999658, n_estimators=144)\n\n\n#---------------- Best Estimator's average RMSE Score on CV (validation set) ----------------#\n\n -0.128184668457489\n\n\n#---------------- Test set results (Best Regressor) ----------------#\n\nMean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R¬≤ Score:\n[0.08217359992105067, 0.01385921018757894, 0.11772514679361815, 0.9173076694801318]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"After going through all steps in RandomSearchCV, we can check the results from it's steps using the \"cvresults\" atrribute."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing all results and metrics, from all models, obtained by the RandomSearchCV steps\ndf_results = pd.DataFrame(best_model_pipeline.cv_results_)\n\ndisplay(df_results)","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"    mean_fit_time  std_fit_time  mean_score_time  std_score_time                         param_data_transformations      param_feature_eng                                          param_reg param_reg__learning_rate param_reg__max_depth param_reg__n_estimators param_reg__max_features param_reg__metric param_reg__n_neighbors param_reg__criterion param_reg__C param_reg__gamma param_reg__eta                                             params  split0_test_neg_mean_absolute_error  split1_test_neg_mean_absolute_error  split2_test_neg_mean_absolute_error  split3_test_neg_mean_absolute_error  mean_test_neg_mean_absolute_error  std_test_neg_mean_absolute_error  rank_test_neg_mean_absolute_error  split0_test_neg_mean_squared_error  split1_test_neg_mean_squared_error  split2_test_neg_mean_squared_error  split3_test_neg_mean_squared_error  mean_test_neg_mean_squared_error  std_test_neg_mean_squared_error  rank_test_neg_mean_squared_error  split0_test_neg_root_mean_squared_error  \\\n0        0.005007      0.000763         0.000000        0.000000  ColumnTransformer(transformers=[('num',\\n     ...  TSNE(n_components=60)  GradientBoostingRegressor(learning_rate=0.0796...                 0.285152                    8                     198                     NaN               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                                  NaN                                  NaN                                  NaN                                  NaN                                NaN                               NaN                                 66                                 NaN                                 NaN                                 NaN                                 NaN                               NaN                              NaN                                66                                      NaN   \n1        1.979473      0.038962         0.438713        0.066680  ColumnTransformer(transformers=[('num',\\n     ...   PCA(n_components=68)                            RandomForestRegressor()                      NaN                    3                      97                    log2               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                            -0.278827                            -0.240363                            -0.236551                            -0.258945                          -0.253671                          0.016814                                 45                           -0.132382                           -0.108747                           -0.094200                           -0.107408                         -0.110684                         0.013757                                45                                -0.363843   \n2        8.892847      0.199848         0.457660        0.078549  ColumnTransformer(transformers=[('num',\\n     ...   PCA(n_components=60)                            RandomForestRegressor()                      NaN                    6                     267                    None               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                            -0.120236                            -0.099669                            -0.111226                            -0.090749                          -0.105470                          0.011197                                  8                           -0.032595                           -0.019239                           -0.025477                           -0.015680                         -0.023248                         0.006436                                 9                                -0.180542   \n3        0.003710      0.000071         0.000000        0.000000  ColumnTransformer(transformers=[('num',\\n     ...  TSNE(n_components=52)                                    LGBMRegressor()                 0.264807                    8                      21                     NaN               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                                  NaN                                  NaN                                  NaN                                  NaN                                NaN                               NaN                                 68                                 NaN                                 NaN                                 NaN                                 NaN                               NaN                              NaN                                68                                      NaN   \n4        1.468835      0.046608         0.484433        0.072346  ColumnTransformer(transformers=[('num',\\n     ...   PCA(n_components=68)                              KNeighborsRegressor()                      NaN                  NaN                     NaN                     NaN         euclidean                      9                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                            -0.137261                            -0.121586                            -0.120497                            -0.113789                          -0.123283                          0.008605                                 20                           -0.042324                           -0.031207                           -0.031178                           -0.023151                         -0.031965                         0.006822                                22                                -0.205728   \n..            ...           ...              ...             ...                                                ...                    ...                                                ...                      ...                  ...                     ...                     ...               ...                    ...                  ...          ...              ...            ...                                                ...                                  ...                                  ...                                  ...                                  ...                                ...                               ...                                ...                                 ...                                 ...                                 ...                                 ...                               ...                              ...                               ...                                      ...   \n95       0.003610      0.000087         0.000000        0.000000  ColumnTransformer(transformers=[('num',\\n     ...  TSNE(n_components=45)  GradientBoostingRegressor(learning_rate=0.0796...                 0.663509                    7                      85                     NaN               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                                  NaN                                  NaN                                  NaN                                  NaN                                NaN                               NaN                                 60                                 NaN                                 NaN                                 NaN                                 NaN                               NaN                              NaN                                60                                      NaN   \n96       7.123167      0.202303         0.529957        0.066889  ColumnTransformer(transformers=[('num',\\n     ...   PCA(n_components=60)  StackingRegressor(estimators=[('svr', SVR(C=10...                      NaN                  NaN                     NaN                     NaN               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                            -0.120539                            -0.098995                            -0.111517                            -0.089549                          -0.105150                          0.011818                                  7                           -0.031500                           -0.018915                           -0.025210                           -0.015289                         -0.022729                         0.006184                                 6                                -0.177483   \n97       4.864886      0.166634         0.432722        0.068791  ColumnTransformer(transformers=[('num',\\n     ...                   None                            RandomForestRegressor()                      NaN                    5                     229                    auto               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                            -0.126591                            -0.117551                            -0.113738                            -0.098408                          -0.114072                          0.010177                                 17                           -0.036814                           -0.026638                           -0.026502                           -0.018298                         -0.027063                         0.006565                                17                                -0.191871   \n98       4.619447      0.518938         0.360319        0.049869  ColumnTransformer(transformers=[('num',\\n     ...                   None                            RandomForestRegressor()                      NaN                    6                     213                    auto               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                            -0.121217                            -0.110719                            -0.107742                            -0.092355                          -0.108008                          0.010331                                 11                           -0.034687                           -0.023513                           -0.024971                           -0.016476                         -0.024912                         0.006494                                13                                -0.186243   \n99       0.003656      0.000212         0.000000        0.000000  ColumnTransformer(transformers=[('num',\\n     ...  TSNE(n_components=60)  StackingRegressor(estimators=[('svr', SVR(C=10...                      NaN                  NaN                     NaN                     NaN               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                                  NaN                                  NaN                                  NaN                                  NaN                                NaN                               NaN                                100                                 NaN                                 NaN                                 NaN                                 NaN                               NaN                              NaN                               100                                      NaN   \n\n    split1_test_neg_root_mean_squared_error  split2_test_neg_root_mean_squared_error  split3_test_neg_root_mean_squared_error  mean_test_neg_root_mean_squared_error  std_test_neg_root_mean_squared_error  rank_test_neg_root_mean_squared_error  split0_test_r2  split1_test_r2  split2_test_r2  split3_test_r2  mean_test_r2  std_test_r2  rank_test_r2  \n0                                       NaN                                      NaN                                      NaN                                    NaN                                   NaN                                     66             NaN             NaN             NaN             NaN           NaN          NaN            66  \n1                                 -0.329768                                -0.306921                                -0.327731                              -0.332066                              0.020409                                     45        0.282700        0.299868        0.313483        0.275772      0.292956     0.014745            45  \n2                                 -0.138704                                -0.159616                                -0.125218                              -0.151020                              0.020993                                      9        0.823385        0.876137        0.814327        0.894276      0.852031     0.033941             9  \n3                                       NaN                                      NaN                                      NaN                                    NaN                                   NaN                                     68             NaN             NaN             NaN             NaN           NaN          NaN            68  \n4                                 -0.176656                                -0.176574                                -0.152155                              -0.177778                              0.018976                                     22        0.770671        0.799083        0.772778        0.843897      0.796607     0.029508            22  \n..                                      ...                                      ...                                      ...                                    ...                                   ...                                    ...             ...             ...             ...             ...           ...          ...           ...  \n95                                      NaN                                      NaN                                      NaN                                    NaN                                   NaN                                     60             NaN             NaN             NaN             NaN           NaN          NaN            60  \n96                                -0.137532                                -0.158775                                -0.123651                              -0.149360                              0.020496                                      6        0.829318        0.878221        0.816277        0.896906      0.855181     0.033370             6  \n97                                -0.163210                                -0.162793                                -0.135269                              -0.163286                              0.020014                                     17        0.800524        0.828502        0.806860        0.876622      0.828127     0.029859            17  \n98                                -0.153339                                -0.158023                                -0.128358                              -0.156491                              0.020548                                     13        0.812053        0.848621        0.818014        0.888907      0.841899     0.030480            13  \n99                                      NaN                                      NaN                                      NaN                                    NaN                                   NaN                                    100             NaN             NaN             NaN             NaN           NaN          NaN           100  \n\n[100 rows x 46 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_data_transformations</th>\n      <th>param_feature_eng</th>\n      <th>param_reg</th>\n      <th>param_reg__learning_rate</th>\n      <th>param_reg__max_depth</th>\n      <th>param_reg__n_estimators</th>\n      <th>param_reg__max_features</th>\n      <th>param_reg__metric</th>\n      <th>param_reg__n_neighbors</th>\n      <th>param_reg__criterion</th>\n      <th>param_reg__C</th>\n      <th>param_reg__gamma</th>\n      <th>param_reg__eta</th>\n      <th>params</th>\n      <th>split0_test_neg_mean_absolute_error</th>\n      <th>split1_test_neg_mean_absolute_error</th>\n      <th>split2_test_neg_mean_absolute_error</th>\n      <th>split3_test_neg_mean_absolute_error</th>\n      <th>mean_test_neg_mean_absolute_error</th>\n      <th>std_test_neg_mean_absolute_error</th>\n      <th>rank_test_neg_mean_absolute_error</th>\n      <th>split0_test_neg_mean_squared_error</th>\n      <th>split1_test_neg_mean_squared_error</th>\n      <th>split2_test_neg_mean_squared_error</th>\n      <th>split3_test_neg_mean_squared_error</th>\n      <th>mean_test_neg_mean_squared_error</th>\n      <th>std_test_neg_mean_squared_error</th>\n      <th>rank_test_neg_mean_squared_error</th>\n      <th>split0_test_neg_root_mean_squared_error</th>\n      <th>split1_test_neg_root_mean_squared_error</th>\n      <th>split2_test_neg_root_mean_squared_error</th>\n      <th>split3_test_neg_root_mean_squared_error</th>\n      <th>mean_test_neg_root_mean_squared_error</th>\n      <th>std_test_neg_root_mean_squared_error</th>\n      <th>rank_test_neg_root_mean_squared_error</th>\n      <th>split0_test_r2</th>\n      <th>split1_test_r2</th>\n      <th>split2_test_r2</th>\n      <th>split3_test_r2</th>\n      <th>mean_test_r2</th>\n      <th>std_test_r2</th>\n      <th>rank_test_r2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005007</td>\n      <td>0.000763</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>TSNE(n_components=60)</td>\n      <td>GradientBoostingRegressor(learning_rate=0.0796...</td>\n      <td>0.285152</td>\n      <td>8</td>\n      <td>198</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>66</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>66</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>66</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>66</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.979473</td>\n      <td>0.038962</td>\n      <td>0.438713</td>\n      <td>0.066680</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>PCA(n_components=68)</td>\n      <td>RandomForestRegressor()</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>97</td>\n      <td>log2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>-0.278827</td>\n      <td>-0.240363</td>\n      <td>-0.236551</td>\n      <td>-0.258945</td>\n      <td>-0.253671</td>\n      <td>0.016814</td>\n      <td>45</td>\n      <td>-0.132382</td>\n      <td>-0.108747</td>\n      <td>-0.094200</td>\n      <td>-0.107408</td>\n      <td>-0.110684</td>\n      <td>0.013757</td>\n      <td>45</td>\n      <td>-0.363843</td>\n      <td>-0.329768</td>\n      <td>-0.306921</td>\n      <td>-0.327731</td>\n      <td>-0.332066</td>\n      <td>0.020409</td>\n      <td>45</td>\n      <td>0.282700</td>\n      <td>0.299868</td>\n      <td>0.313483</td>\n      <td>0.275772</td>\n      <td>0.292956</td>\n      <td>0.014745</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.892847</td>\n      <td>0.199848</td>\n      <td>0.457660</td>\n      <td>0.078549</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>PCA(n_components=60)</td>\n      <td>RandomForestRegressor()</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>267</td>\n      <td>None</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>-0.120236</td>\n      <td>-0.099669</td>\n      <td>-0.111226</td>\n      <td>-0.090749</td>\n      <td>-0.105470</td>\n      <td>0.011197</td>\n      <td>8</td>\n      <td>-0.032595</td>\n      <td>-0.019239</td>\n      <td>-0.025477</td>\n      <td>-0.015680</td>\n      <td>-0.023248</td>\n      <td>0.006436</td>\n      <td>9</td>\n      <td>-0.180542</td>\n      <td>-0.138704</td>\n      <td>-0.159616</td>\n      <td>-0.125218</td>\n      <td>-0.151020</td>\n      <td>0.020993</td>\n      <td>9</td>\n      <td>0.823385</td>\n      <td>0.876137</td>\n      <td>0.814327</td>\n      <td>0.894276</td>\n      <td>0.852031</td>\n      <td>0.033941</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.003710</td>\n      <td>0.000071</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>TSNE(n_components=52)</td>\n      <td>LGBMRegressor()</td>\n      <td>0.264807</td>\n      <td>8</td>\n      <td>21</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>68</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>68</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>68</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.468835</td>\n      <td>0.046608</td>\n      <td>0.484433</td>\n      <td>0.072346</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>PCA(n_components=68)</td>\n      <td>KNeighborsRegressor()</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>euclidean</td>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>-0.137261</td>\n      <td>-0.121586</td>\n      <td>-0.120497</td>\n      <td>-0.113789</td>\n      <td>-0.123283</td>\n      <td>0.008605</td>\n      <td>20</td>\n      <td>-0.042324</td>\n      <td>-0.031207</td>\n      <td>-0.031178</td>\n      <td>-0.023151</td>\n      <td>-0.031965</td>\n      <td>0.006822</td>\n      <td>22</td>\n      <td>-0.205728</td>\n      <td>-0.176656</td>\n      <td>-0.176574</td>\n      <td>-0.152155</td>\n      <td>-0.177778</td>\n      <td>0.018976</td>\n      <td>22</td>\n      <td>0.770671</td>\n      <td>0.799083</td>\n      <td>0.772778</td>\n      <td>0.843897</td>\n      <td>0.796607</td>\n      <td>0.029508</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>0.003610</td>\n      <td>0.000087</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>TSNE(n_components=45)</td>\n      <td>GradientBoostingRegressor(learning_rate=0.0796...</td>\n      <td>0.663509</td>\n      <td>7</td>\n      <td>85</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>7.123167</td>\n      <td>0.202303</td>\n      <td>0.529957</td>\n      <td>0.066889</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>PCA(n_components=60)</td>\n      <td>StackingRegressor(estimators=[('svr', SVR(C=10...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>-0.120539</td>\n      <td>-0.098995</td>\n      <td>-0.111517</td>\n      <td>-0.089549</td>\n      <td>-0.105150</td>\n      <td>0.011818</td>\n      <td>7</td>\n      <td>-0.031500</td>\n      <td>-0.018915</td>\n      <td>-0.025210</td>\n      <td>-0.015289</td>\n      <td>-0.022729</td>\n      <td>0.006184</td>\n      <td>6</td>\n      <td>-0.177483</td>\n      <td>-0.137532</td>\n      <td>-0.158775</td>\n      <td>-0.123651</td>\n      <td>-0.149360</td>\n      <td>0.020496</td>\n      <td>6</td>\n      <td>0.829318</td>\n      <td>0.878221</td>\n      <td>0.816277</td>\n      <td>0.896906</td>\n      <td>0.855181</td>\n      <td>0.033370</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>4.864886</td>\n      <td>0.166634</td>\n      <td>0.432722</td>\n      <td>0.068791</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>None</td>\n      <td>RandomForestRegressor()</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>229</td>\n      <td>auto</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>-0.126591</td>\n      <td>-0.117551</td>\n      <td>-0.113738</td>\n      <td>-0.098408</td>\n      <td>-0.114072</td>\n      <td>0.010177</td>\n      <td>17</td>\n      <td>-0.036814</td>\n      <td>-0.026638</td>\n      <td>-0.026502</td>\n      <td>-0.018298</td>\n      <td>-0.027063</td>\n      <td>0.006565</td>\n      <td>17</td>\n      <td>-0.191871</td>\n      <td>-0.163210</td>\n      <td>-0.162793</td>\n      <td>-0.135269</td>\n      <td>-0.163286</td>\n      <td>0.020014</td>\n      <td>17</td>\n      <td>0.800524</td>\n      <td>0.828502</td>\n      <td>0.806860</td>\n      <td>0.876622</td>\n      <td>0.828127</td>\n      <td>0.029859</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>4.619447</td>\n      <td>0.518938</td>\n      <td>0.360319</td>\n      <td>0.049869</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>None</td>\n      <td>RandomForestRegressor()</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>213</td>\n      <td>auto</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>-0.121217</td>\n      <td>-0.110719</td>\n      <td>-0.107742</td>\n      <td>-0.092355</td>\n      <td>-0.108008</td>\n      <td>0.010331</td>\n      <td>11</td>\n      <td>-0.034687</td>\n      <td>-0.023513</td>\n      <td>-0.024971</td>\n      <td>-0.016476</td>\n      <td>-0.024912</td>\n      <td>0.006494</td>\n      <td>13</td>\n      <td>-0.186243</td>\n      <td>-0.153339</td>\n      <td>-0.158023</td>\n      <td>-0.128358</td>\n      <td>-0.156491</td>\n      <td>0.020548</td>\n      <td>13</td>\n      <td>0.812053</td>\n      <td>0.848621</td>\n      <td>0.818014</td>\n      <td>0.888907</td>\n      <td>0.841899</td>\n      <td>0.030480</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>0.003656</td>\n      <td>0.000212</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>TSNE(n_components=60)</td>\n      <td>StackingRegressor(estimators=[('svr', SVR(C=10...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows √ó 46 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now visualizing all results and metrics obtained only by the best classifier\ndisplay(df_results[df_results['rank_test_neg_root_mean_squared_error'] == 1])","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"   mean_fit_time  std_fit_time  mean_score_time  std_score_time                         param_data_transformations param_feature_eng                                          param_reg param_reg__learning_rate param_reg__max_depth param_reg__n_estimators param_reg__max_features param_reg__metric param_reg__n_neighbors param_reg__criterion param_reg__C param_reg__gamma param_reg__eta                                             params  split0_test_neg_mean_absolute_error  split1_test_neg_mean_absolute_error  split2_test_neg_mean_absolute_error  split3_test_neg_mean_absolute_error  mean_test_neg_mean_absolute_error  std_test_neg_mean_absolute_error  rank_test_neg_mean_absolute_error  split0_test_neg_mean_squared_error  split1_test_neg_mean_squared_error  split2_test_neg_mean_squared_error  split3_test_neg_mean_squared_error  mean_test_neg_mean_squared_error  std_test_neg_mean_squared_error  rank_test_neg_mean_squared_error  split0_test_neg_root_mean_squared_error  \\\n6       3.047665      0.033654         0.420205        0.088948  ColumnTransformer(transformers=[('num',\\n     ...              None  GradientBoostingRegressor(learning_rate=0.0796...                0.0796756                    3                     144                     NaN               NaN                    NaN                  NaN          NaN              NaN            NaN  {'data_transformations': ColumnTransformer(tra...                            -0.097544                            -0.089092                              -0.0959                            -0.072182                           -0.08868                          0.010038                                  1                           -0.023254                           -0.015103                           -0.018699                           -0.010122                         -0.016794                         0.004815                                 1                                -0.152492   \n\n   split1_test_neg_root_mean_squared_error  split2_test_neg_root_mean_squared_error  split3_test_neg_root_mean_squared_error  mean_test_neg_root_mean_squared_error  std_test_neg_root_mean_squared_error  rank_test_neg_root_mean_squared_error  split0_test_r2  split1_test_r2  split2_test_r2  split3_test_r2  mean_test_r2  std_test_r2  rank_test_r2  \n6                                -0.122896                                -0.136743                                -0.100608                              -0.128185                              0.019056                                      1        0.874002        0.902762        0.863726         0.93175       0.89306     0.026527             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_data_transformations</th>\n      <th>param_feature_eng</th>\n      <th>param_reg</th>\n      <th>param_reg__learning_rate</th>\n      <th>param_reg__max_depth</th>\n      <th>param_reg__n_estimators</th>\n      <th>param_reg__max_features</th>\n      <th>param_reg__metric</th>\n      <th>param_reg__n_neighbors</th>\n      <th>param_reg__criterion</th>\n      <th>param_reg__C</th>\n      <th>param_reg__gamma</th>\n      <th>param_reg__eta</th>\n      <th>params</th>\n      <th>split0_test_neg_mean_absolute_error</th>\n      <th>split1_test_neg_mean_absolute_error</th>\n      <th>split2_test_neg_mean_absolute_error</th>\n      <th>split3_test_neg_mean_absolute_error</th>\n      <th>mean_test_neg_mean_absolute_error</th>\n      <th>std_test_neg_mean_absolute_error</th>\n      <th>rank_test_neg_mean_absolute_error</th>\n      <th>split0_test_neg_mean_squared_error</th>\n      <th>split1_test_neg_mean_squared_error</th>\n      <th>split2_test_neg_mean_squared_error</th>\n      <th>split3_test_neg_mean_squared_error</th>\n      <th>mean_test_neg_mean_squared_error</th>\n      <th>std_test_neg_mean_squared_error</th>\n      <th>rank_test_neg_mean_squared_error</th>\n      <th>split0_test_neg_root_mean_squared_error</th>\n      <th>split1_test_neg_root_mean_squared_error</th>\n      <th>split2_test_neg_root_mean_squared_error</th>\n      <th>split3_test_neg_root_mean_squared_error</th>\n      <th>mean_test_neg_root_mean_squared_error</th>\n      <th>std_test_neg_root_mean_squared_error</th>\n      <th>rank_test_neg_root_mean_squared_error</th>\n      <th>split0_test_r2</th>\n      <th>split1_test_r2</th>\n      <th>split2_test_r2</th>\n      <th>split3_test_r2</th>\n      <th>mean_test_r2</th>\n      <th>std_test_r2</th>\n      <th>rank_test_r2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>3.047665</td>\n      <td>0.033654</td>\n      <td>0.420205</td>\n      <td>0.088948</td>\n      <td>ColumnTransformer(transformers=[('num',\\n     ...</td>\n      <td>None</td>\n      <td>GradientBoostingRegressor(learning_rate=0.0796...</td>\n      <td>0.0796756</td>\n      <td>3</td>\n      <td>144</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'data_transformations': ColumnTransformer(tra...</td>\n      <td>-0.097544</td>\n      <td>-0.089092</td>\n      <td>-0.0959</td>\n      <td>-0.072182</td>\n      <td>-0.08868</td>\n      <td>0.010038</td>\n      <td>1</td>\n      <td>-0.023254</td>\n      <td>-0.015103</td>\n      <td>-0.018699</td>\n      <td>-0.010122</td>\n      <td>-0.016794</td>\n      <td>0.004815</td>\n      <td>1</td>\n      <td>-0.152492</td>\n      <td>-0.122896</td>\n      <td>-0.136743</td>\n      <td>-0.100608</td>\n      <td>-0.128185</td>\n      <td>0.019056</td>\n      <td>1</td>\n      <td>0.874002</td>\n      <td>0.902762</td>\n      <td>0.863726</td>\n      <td>0.93175</td>\n      <td>0.89306</td>\n      <td>0.026527</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Feature Importances\n\nIf we want to, it's also possible to check the feature importances of the best model, in case they're easy to understand and explain.\n\nJust remember that, if the best pipeline found in RandomSearchCV applies dimensionality reduction or creates new features using PolynomialFeatures, it will be much harder to explain importances.\n\nIn a scenario that no transformations are applied to the features inside the pipeline, if the model is tree-based (RandomForestClassifier, for example), or linear regression-based (Logistic Regression, for example), then explaining most important features becomes much easier.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, we access the categorical feature names generated by OneHotEncoder, and then concatenate them\n# with the numerical feature names, in the same order our pipeline is applying data transformations.\ncategorical_features_after_onehot = best_model_pipeline.best_estimator_.named_steps['data_transformations']\\\n                                        .transformers_[1][1].named_steps['onehot'].get_feature_names()\n\nfeature_names_in_order = numeric_features + categorical_features_after_onehot\n\nprint(feature_names_in_order)","execution_count":11,"outputs":[{"output_type":"stream","text":"['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', '0_RL', '0_RH', '0_RM', '0_C (all)', '0_FV', '1_Pave', '1_Grvl', '2_Reg', '2_IR1', '2_IR2', '2_IR3', '3_Lvl', '3_HLS', '3_Bnk', '3_Low', '4_AllPub', '4_NoSeWa', '5_Inside', '5_Corner', '5_FR2', '5_CulDSac', '5_FR3', '6_Gtl', '6_Mod', '6_Sev', '7_Somerst', '7_BrkSide', '7_Crawfor', '7_Sawyer', '7_NWAmes', '7_CollgCr', '7_NAmes', '7_NridgHt', '7_MeadowV', '7_OldTown', '7_Gilbert', '7_IDOTRR', '7_Timber', '7_NPkVill', '7_Edwards', '7_Blmngtn', '7_SawyerW', '7_StoneBr', '7_ClearCr', '7_NoRidge', '7_Mitchel', '7_SWISU', '7_Veenker', '7_BrDale', '7_Blueste', '8_Norm', '8_Artery', '8_PosA', '8_Feedr', '8_RRAn', '8_PosN', '8_RRAe', '8_RRNe', '8_RRNn', '9_Norm', '9_PosN', '9_Artery', '9_RRNn', '9_Feedr', '9_RRAe', '9_PosA', '10_1Fam', '10_Duplex', '10_TwnhsE', '10_Twnhs', '10_2fmCon', '11_1Story', '11_2Story', '11_1.5Fin', '11_SLvl', '11_SFoyer', '11_1.5Unf', '11_2.5Fin', '11_2.5Unf', '12_Gable', '12_Hip', '12_Gambrel', '12_Mansard', '12_Flat', '12_Shed', '13_CompShg', '13_Tar&Grv', '13_ClyTile', '13_WdShake', '13_WdShngl', '13_Membran', '13_Roll', '14_VinylSd', '14_Wd Sdng', '14_MetalSd', '14_HdBoard', '14_CemntBd', '14_AsbShng', '14_BrkFace', '14_Stucco', '14_Plywood', '14_WdShing', '14_CBlock', '14_Stone', '14_BrkComm', '14_ImStucc', '15_VinylSd', '15_Wd Sdng', '15_MetalSd', '15_HdBoard', '15_CmentBd', '15_AsbShng', '15_Wd Shng', '15_Plywood', '15_Stucco', '15_BrkFace', '15_Stone', '15_ImStucc', '15_CBlock', '15_Brk Cmn', '15_AsphShn', '15_Other', '16_Stone', '16_None', '16_BrkFace', '16_BrkCmn', '17_Gd', '17_TA', '17_Ex', '17_Fa', '18_TA', '18_Gd', '18_Po', '18_Fa', '18_Ex', '19_PConc', '19_CBlock', '19_BrkTil', '19_Slab', '19_Stone', '19_Wood', '20_Ex', '20_TA', '20_Gd', '20_Fa', '21_TA', '21_Gd', '21_Fa', '21_Po', '22_Av', '22_Gd', '22_No', '22_Mn', '23_GLQ', '23_Rec', '23_Unf', '23_ALQ', '23_LwQ', '23_BLQ', '24_Unf', '24_BLQ', '24_LwQ', '24_ALQ', '24_Rec', '24_GLQ', '25_GasA', '25_GasW', '25_Grav', '25_OthW', '25_Wall', '25_Floor', '26_Ex', '26_TA', '26_Gd', '26_Fa', '27_Y', '27_N', '28_SBrkr', '28_FuseF', '28_FuseA', '28_FuseP', '28_Mix', '29_Gd', '29_TA', '29_Ex', '29_Fa', '30_Typ', '30_Maj1', '30_Min1', '30_Min2', '30_Mod', '30_Maj2', '30_Sev', '31_Gd', '31_TA', '31_Ex', '31_Fa', '31_Po', '32_Attchd', '32_Detchd', '32_BuiltIn', '32_2Types', '32_CarPort', '32_Basment', '33_RFn', '33_Unf', '33_Fin', '34_TA', '34_Fa', '34_Gd', '34_Ex', '34_Po', '35_TA', '35_Fa', '35_Gd', '35_Po', '35_Ex', '36_Y', '36_P', '36_N', '37_8.0', '37_5.0', '37_1.0', '37_7.0', '37_4.0', '37_12.0', '37_11.0', '37_10.0', '37_6.0', '37_2.0', '37_9.0', '37_3.0', '38_2007.0', '38_2009.0', '38_2006.0', '38_2008.0', '38_2010.0', '39_WD', '39_COD', '39_New', '39_ConLI', '39_CWD', '39_Con', '39_ConLw', '39_ConLD', '39_Oth', '40_Normal', '40_Abnorml', '40_Partial', '40_Family', '40_Alloca', '40_AdjLand']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting feature importances from LGBM Regressor\n# plot_importance_lgbm(best_model_pipeline.best_estimator_[2], figsize=(10, 14))\n\n\n# Plotting feature importances from XGB Regressor\n# plot_importance_xgb(best_model_pipeline.best_estimator_[2], figsize=(10, 14))\n\n\n# # Plotting feature importances of the best model, if linear regression-based (top 5 features)\n# print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n# feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['reg'].coef_[0], index=feature_names_in_order)\n# feat_importances.nlargest(5).plot(kind='barh')\n\n\n# Plotting feature importances of the best model, if sklearn tree-based (top 5 features)\nprint(\"\\n#---------------- Bar plot with feature importances ----------------#\")\nfeat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['reg'].feature_importances_, index=feature_names_in_order)\nfeat_importances.nlargest(5).plot(kind='barh')\n","execution_count":12,"outputs":[{"output_type":"stream","text":"\n#---------------- Bar plot with feature importances ----------------#\n","name":"stdout"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f1090cdcb50>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Predictions\n\nNow that we have tried different preprocessing and modeling techniques, resulting in a final best pipeline, let's use it to predict the test data provided by kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the data and displaying some rows\ndf_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\n# Dropping the unnecessary columns\ndf_test.drop(['Id', 'Alley','PoolQC','Fence','MiscFeature'], axis=1, inplace=True)\n\n# Applying best_model_pipeline:\n# Step 1 -> Transforming data the same way we did in the training set;\n# Step 2 -> making predictions using the best model obtained by RandomSearchCV.\ntest_predictions = best_model_pipeline.predict(df_test)\n\n# Because our model was trained using a logarithmic scale of the target, it's predictions will also\n# be log. We need to get them back to linear scale using np.exp()\ntest_predictions = np.exp(test_predictions)\nprint(test_predictions)","execution_count":13,"outputs":[{"output_type":"stream","text":"[126435.68465769 158288.35833123 180186.04653891 ... 154837.27390704\n 123999.76574656 239463.18777739]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the predictions file that is going to be submitted to the competition\ndf_submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\ndf_submission['SalePrice'] = test_predictions # Adding a column with predicted values\n\ndf_submission.drop(df_submission.columns.difference(['Id', 'SalePrice']), axis=1, inplace=True) # Selecting only needed columns\n\ndf_submission.head(10)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"     Id      SalePrice\n0  1461  126435.684658\n1  1462  158288.358331\n2  1463  180186.046539\n3  1464  188244.547176\n4  1465  190584.917481\n5  1466  173356.682206\n6  1467  177394.338589\n7  1468  165785.632883\n8  1469  189481.425706\n9  1470  124453.609634","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1461</td>\n      <td>126435.684658</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1462</td>\n      <td>158288.358331</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1463</td>\n      <td>180186.046539</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1464</td>\n      <td>188244.547176</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1465</td>\n      <td>190584.917481</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1466</td>\n      <td>173356.682206</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1467</td>\n      <td>177394.338589</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1468</td>\n      <td>165785.632883</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1469</td>\n      <td>189481.425706</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1470</td>\n      <td>124453.609634</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if the number of rows is OK (the file is expected to have 1459 rows)\ndf_submission.count()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"Id           1459\nSalePrice    1459\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing submitions to CSV file\ndf_submission.to_csv('submission.csv', index=False)","execution_count":16,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}