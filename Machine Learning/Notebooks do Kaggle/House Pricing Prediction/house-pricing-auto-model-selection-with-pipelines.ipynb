{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Pricing: Automatic Data preprocessing & Modeling Techniques Selection using Pipelines\n\nNotebook written by Pedro de Matos Gonçalves","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport collections\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 100) # Setting pandas to display a N number of columns\npd.set_option('display.max_rows', 10) # Setting pandas to display a N number rows\npd.set_option('display.width', 1000) # Setting pandas dataframe display width to N\nimport matplotlib.pyplot as plt # data visualization library\nimport plotly.graph_objs as go # interactive plotting library\nfrom IPython.display import display # display from IPython.display\nfrom itertools import cycle # function used for cycling over values\n\n\n# Libraries used for Modeling\nfrom scipy import stats\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom category_encoders import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, StackingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom xgboost import XGBRegressor, plot_importance\nfrom lightgbm import LGBMRegressor, plot_importance, plot_tree, create_tree_digraph\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing the data and displaying some rows\ndf = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n\ndisplay(df.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for columns where null values are higher than 50% of it's total\ndf_aux_nulls = [(c, df[c].isna().mean()*100) for c in df]\ndf_aux_nulls = pd.DataFrame(df_aux_nulls, columns=[\"column_name\", \"null_percentage\"])\n\ndf_aux_nulls = df_aux_nulls[df_aux_nulls.null_percentage > 50]\nprint(\"Columns with more than 50% null percentage:\")\ndf_aux_nulls.sort_values(\"null_percentage\", ascending=False) # These are the 3 columns with more than 50% of nulls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming our target into log scale, to help improve generalization\ndf['SalePrice'] = np.log(df.SalePrice)\n\n# Separating our target\ntarget = df['SalePrice']\n\n# Let's drop the highly null columns and our target from the original dataframe.\ndf.drop(['Id', 'Alley','PoolQC','Fence','MiscFeature', 'SalePrice'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking sell month and year as categorical columns\ndf['YrSold'].astype('object')\ndf['MoSold'].astype('object')\n\n# Now, we separate categorical and numerical column dataframes.\ncategorical_df = df.select_dtypes(include=['object'])\nnumeric_df = df.select_dtypes(exclude=['object'])\n\n# And then, we store the names of the categorical and numerical columns.\ncategorical_features = list(categorical_df.columns)\nnumeric_features = list(numeric_df.columns)\n\nprint(\"Categorical features:\\n\", categorical_features)\nprint(\"\\nNumeric features:\\n\", numeric_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Note: The main goal of this cell is to simulate the transformations that a pipeline would do in our features \n# (mainly, on the categorical ones), so we can take a look on the column order after OneHotEncoding.\nohe = OneHotEncoder(use_cat_names=True) # Creating the one-hot encoder object\ndf_onehotencoded = ohe.fit_transform(df) # Transforming our original data\n\n# Reorganizing columns the same way pipeline will be set\n# Our pipeline will be constructed following in logic: \n# -> 1st: Deal with numeric columns. \n# -> 2nd: Then, Categorical columns.\ndf_onehotencoded.drop(numeric_features, axis=1, inplace=True) \ndf_all_features = pd.concat([df[numeric_features], df_onehotencoded], axis=1)\n\ndisplay(df_all_features.head(10)) # Taking a look how the dataset looks after being passed through the pipeline\n\n\n# We are also going to store the names of the features in the correct order for plotting Feature Importances later.\nfeature_names = list(df_all_features.columns) # Storing feature names in the correct order\n\nfeature_names = [item for item in feature_names if '_nan' not in item] # Null values are filled with Imputer inside our pipeline,\n                                                                       # so \"_nan\" columns will not be in our final dummies created\n                                                                       # by the OneHotEncoder object.\n    \nprint(\"Feature names after Pipeline transformation (numeric ones first, then categorical ones):\\n\\n\", feature_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## Model training & Evaluation functions\n\nAfter all the preprocessing, we are now ready for building and evaluating different Machine Learning models.\n\nFirst, let's create a function responsible for evaluating our regressor on the test set we have created.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def testSetResultsRegressor(regressor, x_test, y_test):\n    predictions = regressor.predict(x_test)\n    \n    results = []\n    \n    mae = mean_absolute_error(y_test, predictions)\n    mse = mean_squared_error(y_test, predictions)\n    rmse = mean_squared_error(y_test, predictions, squared=False)\n    r2 = r2_score(y_test, predictions)\n    \n    results.append(mae)\n    results.append(mse)\n    results.append(rmse)\n    results.append(r2)\n    \n    print(\"\\n\\n#---------------- Test set results (Best Regressor) ----------------#\\n\")\n    print(\"Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R² Score:\")\n    print(results)\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we fit several different data preprocessing, feature selection and modeling techniques inside a Pipeline, to check which group of techniques has better performance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building a Pipeline responsible for finding best model and it's parameters\ndef defineBestModelPipeline(df, target, categorical_features, numeric_features):\n    \n    # Splitting data into Train and Test\n    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.2, random_state=1)\n    y_train = y_train.to_numpy() # Transforming training targets into numpy arrays\n    y_test = y_test.to_numpy() # Transforming test targets into numpy arrays\n    \n    \n    # Pipeline's data transformations\n    # 1st -> Numeric Transformers (we'll try several different ones)\n    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=10, random_state=1)),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=7)),\n                                            ('scaler', StandardScaler())])\n    \n    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n    \n    \n    # 2nd -> Categorical Transformer\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n    \n    # 3rd -> Different Data Transformation Steps, each one with a different numerical transformation\n    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_features),\n                                                             ('cat', categorical_transformer, categorical_features)])\n    \n    \n    \n    # Applying different data transformations in RandomSearchCV to find \n    # the best imputing strategy, the best feature engineering strategy\n    # and the best model with it's parameters\n    pipe = Pipeline(steps=[('data_transformations', data_transformations_1), # Initializing data transformation step by choosing any of the above\n                           ('feature_eng', PCA()), # Initializing feature engineering step by choosing any desired method\n                           ('reg', SVR())]) # Initializing modeling step of the pipeline with any model object\n                           #memory='cache_folder') -> Used to optimize memory when needed\n    \n    \n    \n    # Now, defining the grid of parameters to search for. RandomSearchCV will randomly chose\n    # options for each step inside the dictionaries, and return the best one for us as our final pipeline.\n    params_grid = [\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [KNeighborsRegressor()],\n                     'reg__n_neighbors': stats.randint(1, 30),\n                     'reg__metric': ['minkowski', 'euclidean']},\n\n        \n\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [LinearRegression()]},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [SVR()],\n                     'reg__C': stats.uniform(0.01, 10),\n                     'reg__gamma': stats.uniform(0.01, 10)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [DecisionTreeRegressor()],\n                     'reg__criterion': ['gini', 'entropy'],\n                     'reg__max_features': [None, \"auto\", \"log2\"],\n                     'reg__max_depth': stats.randint(1, 7)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [RandomForestRegressor()],\n                     'reg__n_estimators': stats.randint(10, 300),\n                     'reg__max_features': [None, \"auto\", \"log2\"],\n                     'reg__max_depth': stats.randint(1, 7)},\n        \n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [ExtraTreesRegressor()],\n                     'reg__n_estimators': stats.randint(10, 300),\n                     'reg__max_features': [None, \"auto\", \"log2\"],\n                     'reg__max_depth': stats.randint(1, 7)},\n\n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [GradientBoostingRegressor()],\n                     'reg__n_estimators': stats.randint(10, 200),\n                     'reg__learning_rate': stats.uniform(0.01, 1.2),\n                     'reg__max_depth': stats.randint(1, 9)},\n\n        \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [LGBMRegressor()],\n                     'reg__n_estimators': stats.randint(1, 150),\n                     'reg__learning_rate': stats.uniform(0.01, 1),\n                     'reg__max_depth': stats.randint(1, 5)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [XGBRegressor()],\n                     'reg__n_estimators': stats.randint(1, 125),\n                     'reg__eta': stats.uniform(0.01, 1),\n                     'reg__max_depth': stats.randint(1, 8),\n                     'reg__gamma': stats.uniform(0.01, 1)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [StackingRegressor(estimators=[('svr', SVR(C=10, gamma=10)),\n                                                           ('rf', RandomForestRegressor(max_depth=5, max_features=None, n_estimators=50, n_jobs=-1)),\n                                                           ('xgb', XGBRegressor(eta=0.5, gamma=0.5, max_depth=4, n_estimators=25))], \n                                                final_estimator=LinearRegression())]},\n        \n        \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(df_all_features.shape[1]*0.9)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.7)),\n                                     PCA(n_components=round(df_all_features.shape[1]*0.5)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.8)),\n                                     TSNE(n_components=round(df_all_features.shape[1]*0.6)), \n                                     TSNE(n_components=round(df_all_features.shape[1]*0.4))],\n                     'reg': [StackingRegressor(estimators=[('lgbm', LGBMRegressor(n_estimators=30, learning_rate=0.4, max_depth=6)),\n                                                           ('etc', ExtraTreesRegressor(max_depth=6, max_features=None, n_estimators=30)),\n                                                           ('gbt', GradientBoostingRegressor(learning_rate=0.6, max_depth=5, n_estimators=15))], \n                                                final_estimator=LinearRegression())]}\n                ]\n    \n    \n    # Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n    metrics = ['neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_root_mean_squared_error', 'r2']\n    \n    best_model_pipeline = RandomizedSearchCV(pipe, params_grid, n_iter=50, scoring=metrics, \n                                             refit='neg_root_mean_squared_error', \n                                             n_jobs=-1, cv=5, random_state=1)\n\n    best_model_pipeline.fit(x_train, y_train)\n    \n    \n    # At last, we check the final results\n    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n    print(\"\\n\\n#---------------- Best Regressor found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n    print(\"\\n\\n#---------------- Best Estimator's average RMSE Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n    \n    return x_train, x_test, y_train, y_test, best_model_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calling the function above, returing train/test data and best model's pipeline\nx_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(df, target, categorical_features, numeric_features)\n\n\n# Checking best model's performance on test data\ntest_set_results = testSetResultsRegressor(best_model_pipeline, x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After going through all steps in RandomSearchCV, we can check the results from it's steps using the \"cvresults\" atrribute","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing all results and metrics, from all models, obtained by the RandomSearchCV steps\ndf_results = pd.DataFrame(best_model_pipeline.cv_results_)\n\ndisplay(df_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now visualizing all results and metrics obtained only by the best classifier\ndisplay(df_results[df_results['rank_test_neg_root_mean_squared_error'] == 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we want to, it's also possible to check the feature importances of the best model, in case they're easy to understand and explain.\n\nJust remember that, if the best pipeline found in RandomSearchCV applies dimensionality reduction or creates new features using PolynomialFeatures, it will be much harder to explain importances.\n\nIn a scenario that no transformations are applied to the features inside the pipeline, if the model is tree-based (RandomForestClassifier, for example), or linear regression-based (Logistic Regression, for example), then explaining most important features becomes much easier.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Plotting feature importances of the best model, if tree-based (top 5 features)\n# print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n# feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['reg'].feature_importances_, index=feature_names)\n# feat_importances.nlargest(5).plot(kind='barh')\n\n\n# # Plotting feature importances of the best model, if linear regression-based (top 5 features)\n# print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n# feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['reg'].coef_[0], index=feature_names)\n# feat_importances.nlargest(5).plot(kind='barh')\n\n\n# Plotting feature importances from LGBM Regressor\nplot_importance(best_model_pipeline.best_estimator_[2], figsize=(10, 14))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Predictions\n\nNow that we have tried different preprocessing and modeling techniques, resulting in a final best pipeline, let's use it to predict the test data provided by kaggle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the data and displaying some rows\ndf_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\n# Dropping the unnecessary columns\ndf_test.drop(['Id', 'Alley','PoolQC','Fence','MiscFeature'], axis=1, inplace=True)\n\n# Applying best_model_pipeline:\n# Step 1 -> Transforming data the same way we did in the training set;\n# Step 2 -> making predictions using the best model obtained by RandomSearchCV.\ntest_predictions = best_model_pipeline.predict(df_test)\n\n# Because our model was trained using a logarithmic scale of the target, it's predictions will also\n# be log. We need to get them back to linear scale using np.exp()\ntest_predictions = np.exp(test_predictions)\nprint(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generating the predictions file that is going to be submitted to the competition\ndf_submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\ndf_submission['SalePrice'] = test_predictions # Adding a column with predicted values\n\ndf_submission.drop(df_submission.columns.difference(['Id', 'SalePrice']), axis=1, inplace=True) # Selecting only needed columns\n\ndf_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if the number of rows is OK (the file is expected to have 1459 rows)\ndf_submission.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Writing submitions to CSV file\ndf_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}